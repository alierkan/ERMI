# -*- coding: utf-8 -*-
"""parseme002.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f3mV80S68q-Y1HNzCzKQCqvH79TtNUOe
"""

import os
import argparse

from numpy.random import seed
seed(17)
from tensorflow import set_random_seed
set_random_seed(23)

from MWEPreProcessor import MWEPreProcessor
from Operations import load_pickle, get_logger, dump_pickle
from MWEIdentifier import MWEIdentifier

parser = argparse.ArgumentParser(prog='CICLing_42')
parser.add_argument('-l', '--lang')  # -l EN
parser.add_argument('-t', '--tag')  # -t IOB
parser.add_argument('-e', '--embed')  # -h IOB
parser.add_argument('-v', '--version')  # -v 001, 002
parser.add_argument('-w', '--withraw') # -w yes, no

args = parser.parse_args()
lang = args.lang.upper()
tag = args.tag
embedding_type = args.embed
trial_version = args.version
withraw = args.withraw

print(lang, ', ', tag, ', ', embedding_type, ', ', trial_version, ', ', withraw)

root_path = os.getcwd()
root_path = os.path.join(root_path, '..')
input_path = os.path.join(root_path, 'input', 'corpora', 'sharedtask-data-master', '1.2')
output_path = os.path.join(root_path, 'output', lang)
gensim_name = "gensim_" + lang.lower()
gensim_we_path = os.path.join(root_path, 'input', 'embeddings', lang, gensim_name)

mwe_path = os.path.join(input_path, lang)
write_folder_name = 'ERMI_' + trial_version
mwe_write_path = os.path.join(mwe_path, write_folder_name)

if not os.path.exists(mwe_write_path):
    os.makedirs(mwe_write_path)

logger = get_logger(mwe_write_path)

if not os.path.exists(output_path):
    os.makedirs(output_path)

# preprocessing
logger.info('Preparing mwe...')
mwepp = MWEPreProcessor(lang, withraw, input_path, mwe_write_path)
logger.info('set_tagging...')
mwepp.set_tagging(tag)
logger.info('set_train_dev...')
mwepp.set_train_dev()
logger.info('tag...')
mwepp.tag()

mwe_train_path = os.path.join(mwe_write_path, 'train.pkl')
mwe_test_path = os.path.join(mwe_write_path, 'test.pkl')
mwe_model_path = os.path.join(mwe_write_path, 'model.pkl')

logger.info('set_test_corpus...')
mwepp.set_test_corpus()
logger.info('update_test_corpus...')
mwepp.update_test_corpus()
"""logger.info('set_raw_corpus...')
mwepp.set_raw_corpus(raw_corpus)"""
logger.info('prepare_to_lstm...')
mwepp.prepare_to_lstm()
logger.info('dump_pickle...')
dump_pickle(mwepp, mwe_train_path)

"""**Load Gensim Fasttext Embeddings**"""

from gensim.models.fasttext import FastText as FT_gensim

logger.info('Reading word embedding...')
model_gensim = FT_gensim.load(gensim_we_path)
print(model_gensim)

def set_gensim_word_embeddings(mwe_train_path):
    mwe = load_pickle(mwe_train_path)

    embedding_size = 300
    embedding_matrix = np.zeros((mwe.n_words, embedding_size))
    unk_word_vector = [round(np.random.uniform(-0.1, 0.1), 5) for i in range(300)]
    unk_word_count = 0

    for vocab_idx, word in enumerate(mwe.words):
        if word == '<UNK>':
            continue
        else:
          try:
            embedding_vector = model_gensim[word]
          except KeyError:
            embedding_vector = unk_word_vector
            unk_word_count += 1
          embedding_matrix[vocab_idx] = np.array(embedding_vector)

    mwe.set_word_embeddings(embedding_matrix)
    mwe.unk_word_vector = unk_word_vector
    dump_pickle(mwe, mwe_train_path)

    print("Vocab size: ", len(mwe.words), " , Unk count: ", unk_word_count)
    return embedding_matrix

import numpy as np

gensim_word_embeddings = set_gensim_word_embeddings(mwe_train_path)

"""**Set test set & training parameters**"""

# model
params = {'DE': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 16, 'epochs': 20},
          'EL': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 32, 'epochs': 20},
          'EU': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 16, 'epochs': 15},
          'FR': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 32, 'epochs': 20},
          'GA': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 8, 'epochs': 12},
          'HE': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 32, 'epochs': 20},
          'HI': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 8, 'epochs': 12},
          'IT': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 16, 'epochs': 15},
          'PL': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 32, 'epochs': 20},
          'PT': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 32, 'epochs': 20},
          'RO': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 16, 'epochs': 15},
          'SV': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 8, 'epochs': 12},
          'TR': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 32, 'epochs': 20},
          'ZH': {'n_units': 20, 'dropout': [0.1, 0.1], 'batch_size': 32, 'epochs': 20}
          }

logger.info('Reading mwe pkl...')
mwe = load_pickle(mwe_train_path)
mwe_identifier = MWEIdentifier(lang, embedding_type, mwe, logger, mwe_write_path)
mwe_identifier.set_params(params[lang])
logger.info('set_test...')
mwe_identifier.set_test()

logger.info('build_model...')
mwe_identifier.build_model()
logger.info('fit_model...')
mwe_identifier.fit_model()
logger.info('predict...')
mwe_identifier.predict()
logger.info('add_tags_to_test...')
#mwe_identifier.mwe.test_tagged_path = os.path.join(mwe_identifier.mwe.root_path, "test_tagged_001.cupt")
mwe_identifier.add_tags_to_test()
logger.info('Saving model...')
mwe_identifier.model.save(mwe_model_path)
dump_pickle(mwe_identifier.mwe, mwe_test_path)

"""**Save teacher model**"""

mwe_teacher_model_path = os.path.join(mwe_write_path, 'teacher_model')
mwepp_path = os.path.join(mwe_write_path, 'mwepp.pkl')

logger.info('Saving teacher model...')
mwe_identifier.model.save(mwe_teacher_model_path)
#dump_pickle(mwe_identifier, mwe_identifier_path)
dump_pickle(mwepp, mwepp_path)